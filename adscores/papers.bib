
@inproceedings{bergmann_mvtec_2019,
	title = {{MVTec} {AD} – {A} {Comprehensive} {Real}-{World} {Dataset} for {Unsupervised} {Anomaly} {Detection}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	month = jun,
	year = {2019},
}

@inproceedings{schlegl_unsupervised_2017,
	address = {Cham},
	title = {Unsupervised {Anomaly} {Detection} with {Generative} {Adversarial} {Networks} to {Guide} {Marker} {Discovery}},
	isbn = {978-3-319-59050-9},
	abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
	editor = {Niethammer, Marc and Styner, Martin and Aylward, Stephen and Zhu, Hongtu and Oguz, Ipek and Yap, Pew-Thian and Shen, Dinggang},
	year = {2017},
	keywords = {verified},
	pages = {146--157},
}

@article{napoletano_anomaly_2018,
	title = {Anomaly {Detection} in {Nanofibrous} {Materials} by {CNN}-{Based} {Self}-{Similarity}},
	volume = {18},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5795842/},
	doi = {10.3390/s18010209},
	abstract = {Automatic detection and localization of anomalies in nanofibrous materials help to reduce the cost of the production process and the time of the post-production visual inspection process. Amongst all the monitoring methods, those exploiting Scanning Electron Microscope (SEM) imaging are the most effective. In this paper, we propose a region-based method for the detection and localization of anomalies in SEM images, based on Convolutional Neural Networks (CNNs) and self-similarity. The method evaluates the degree of abnormality of each subregion of an image under consideration by computing a CNN-based visual similarity with respect to a dictionary of anomaly-free subregions belonging to a training set. The proposed method outperforms the state of the art.},
	number = {1},
	urldate = {2022-02-02},
	journal = {Sensors (Basel, Switzerland)},
	author = {Napoletano, Paolo and Piccoli, Flavio and Schettini, Raimondo},
	month = jan,
	year = {2018},
	pmid = {29329268},
	pmcid = {PMC5795842},
	pages = {209},
}

@inproceedings{liu_towards_2020,
	title = {Towards {Visually} {Explaining} {Variational} {Autoencoders}},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Liu, Wenqian and Li, Runze and Zheng, Meng and Karanam, Srikrishna and Wu, Ziyan and Bhanu, Bir and Radke, Richard J. and Camps, Octavia},
	month = jun,
	year = {2020},
	keywords = {verified},
}

@inproceedings{li_superpixel_2020,
	title = {Superpixel {Masking} and {Inpainting} for {Self}-{Supervised} {Anomaly} {Detection}},
	abstract = {Anomaly detection aims at identifying abnormal samples from the normal ones. Existing methods are usually supervised or detect anomalies at the instance level without localization. In this work, we propose an unsupervised method called Superpixel Masking And Inpainting (SMAI) to identify and locate anomalies in images. Speciﬁcally, superpixel segmentation is ﬁrst performed on the images. Then an inpainting module is trained to learn the spatial and texture information of the normal samples through random superpixel masking and restoration. Therefore, the model can reconstruct the superpixel mask with normal content. At the inference stage, we mask the image using superpixels and restore them one by one. By comparing the mask areas of the original image and its reconstruction, we can identify and locate the abnormal regions. We conducted a comprehensive evaluation of SMAI on the latest MVTec anomaly detection dataset, and it shows that SMAI plays favorably against state-of-the-art methods.},
	language = {en},
	author = {Li, Zhenyu and Li, Ning and Jiang, Kaitao and Ma, Zhiheng and Wei, Xing and Hong, Xiaopeng and Gong, Yihong},
	year = {2020},
	pages = {12},
}

@inproceedings{dehaene_iterative_2020,
	title = {Iterative energy-based projection on a normal data manifold for anomaly localization},
	abstract = {Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder’s loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-deﬁned optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset.},
	language = {en},
	author = {Dehaene, David and Frigo, Oriel and Combrexelle, Sébastien and Eline, Pierre},
	year = {2020},
	pages = {17},
}

@inproceedings{zhou_encoding_2020,
	address = {Cham},
	title = {Encoding {Structure}-{Texture} {Relation} with {P}-{Net} for {Anomaly} {Detection} in {Retinal} {Images}},
	isbn = {978-3-030-58565-5},
	abstract = {Anomaly detection in retinal image refers to the identification of abnormality caused by various retinal diseases/lesions, by only leveraging normal images in training phase. Normal images from healthy subjects often have regular structures (e.g., the structured blood vessels in the fundus image, or structured anatomy in optical coherence tomography image). On the contrary, the diseases and lesions often destroy these structures. Motivated by this, we propose to leverage the relation between the image texture and structure to design a deep neural network for anomaly detection. Specifically, we first extract the structure of the retinal images, then we combine both the structure features and the last layer features extracted from original health image to reconstruct the original input healthy image. The image feature provides the texture information and guarantees the uniqueness of the image recovered from the structure. In the end, we further utilize the reconstructed image to extract the structure and measure the difference between structure extracted from original and the reconstructed image. On the one hand, minimizing the reconstruction difference behaves like a regularizer to guarantee that the image is corrected reconstructed. On the other hand, such structure difference can also be used as a metric for normality measurement. The whole network is termed as P-Net because it has a “P” shape. Extensive experiments on RESC dataset and iSee dataset validate the effectiveness of our approach for anomaly detection in retinal images. Further, our method also generalizes well to novel class discovery in retinal images and anomaly detection in real-world images.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhou, Kang and Xiao, Yuting and Yang, Jianlong and Cheng, Jun and Liu, Wen and Luo, Weixin and Gu, Zaiwang and Liu, Jiang and Gao, Shenghua},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {360--377},
}

@inproceedings{liznerski_explainable_2021,
	title = {Explainable {Deep} {One}-{Class} {Classification}},
	url = {https://openreview.net/forum?id=A5VV3UyIQz},
	abstract = {Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is...},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liznerski, Philipp and Ruff, Lukas and Vandermeulen, Robert A. and Franks, Billy Joe and Kloft, Marius and Muller, Klaus Robert},
	year = {2021},
	keywords = {verified},
}
